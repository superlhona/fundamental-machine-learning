{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song genre prediction\n",
    "<b>Predict a song genre using its track information and features</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, my research question is: <b>Can we predict a track genre based on track info and features?</b><br>\n",
    "<br>\n",
    "Through its API, Spotify provided users with variety of track features, from tempo and key to whether the song is with lyrics or not and whether the song is suitable for dancing or not. These track features will be the X variables. Using these track features, I will predict the genre of the song. This also means, that genre is the Y variable.<br>\n",
    "<br>\n",
    "The unit of observation of this project is song. I will use 6 Spotify playlists from 6 different era: All Out 50s, All Out 60s, All Out 70s, All Out 80s, All Out 90s, and All Out 00s. Each playlist consist of minimum 100 songs.<br>\n",
    "<br>\n",
    "For this proejctI will use <a href=\"https://github.com/plamere/spotipy\">Spotipy</a> library to get data from Spotify API. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will firstly get the track IDs from all the playlist. Then, I from each track ID, will get the track features. From the Artist data, I will then get the possible genre of the song. Why possible genre? Because in Spotify, the genre is related to the artist, not the song. Therefore, each song might be tagged with multiple genre. These list of genre that is a categorical variables then will be transformed into dummy variables.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries for this project, then connect with my Spotify for Developer credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from pprint import pprint\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn as sk\n",
    "\n",
    "sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id='eaa563769e874ccd85c8185f682ef625', client_secret='9d60c77a99e34791a1b126a06d43dcf6'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the playlist IDs I'm going to work with. Since I hand-picked the playlist through Spotify Web Player, I was able to get the playlist ID through Spotify Web Player. Another way of doing this is to get a list of a user's playlist and search for the playlist IDs.<br>\n",
    "<br>\n",
    "In the below array, the playlist are sorted by oldest to newest (50s to 00s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = ['37i9dQZF1DWSV3Tk4GO2fq', '37i9dQZF1DXaKIA8E7WcJj', '37i9dQZF1DWTJ7xPn4vNaz', '37i9dQZF1DX4UtSsGT1Sbe', '37i9dQZF1DXbTxeAdrVG2l', '37i9dQZF1DX4o1oenSJRJd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, we're getting the track IDs of every song on the above playlists and save them in a csv file, so that we can work with the data locally in the next process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source this\n",
    "get_tracks = []\n",
    "\n",
    "for list in playlists:\n",
    "    offset = 0    \n",
    "    \n",
    "    while True:\n",
    "        response = sp.playlist_items(list,\n",
    "                                     offset=offset,\n",
    "                                     fields='items.track.id,total',\n",
    "                                     additional_types=['track'])\n",
    "\n",
    "        if len(response['items']) == 0:\n",
    "            break\n",
    "        \n",
    "        get_tracks.extend(response['items'])\n",
    "        offset = offset + len(response['items'])\n",
    "        \n",
    "\n",
    "with open(\"tracks.json\", \"w\") as outfile:\n",
    "    json.dump(get_tracks, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, we take the tracks data stored locally and continue work with it. The data was saved as json file, we're going to transform it into a dataframe (df) for further working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06PdA0DLgF4BfAeUNZAbFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40KCF1U2DNceYsugxZQruU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2viNxee3uNcVXrXvwAUVir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72nqbbrKjhXmDdRXQGq115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50RwvvEKX5Q7AZt8FTAwIF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id\n",
       "0  06PdA0DLgF4BfAeUNZAbFG\n",
       "1  40KCF1U2DNceYsugxZQruU\n",
       "2  2viNxee3uNcVXrXvwAUVir\n",
       "3  72nqbbrKjhXmDdRXQGq115\n",
       "4  50RwvvEKX5Q7AZt8FTAwIF"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"tracks.json\")\n",
    "df = pd.json_normalize(data['track'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we take the track names, artist names and artist ids from the track id data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source this\n",
    "tracks = df['id']\n",
    "track_names = []\n",
    "track_artists_name = []\n",
    "track_artists_id = []\n",
    "\n",
    "for track in tracks:\n",
    "    urn = 'spotify:track:' + track\n",
    "    track = sp.track(urn)\n",
    "    track_name = track['name']\n",
    "    track_artist_name = track['artists'][0]['name']\n",
    "    track_artist_id = track['artists'][0]['id']\n",
    "    track_explicit = track['explicit']\n",
    "    track_album_type = track['album']['album_type']\n",
    "    track_names.append(track_name)\n",
    "    track_artists_name.append(track_artist_name)\n",
    "    track_artists_id.append(track_artist_id)\n",
    "    \n",
    "df['track names'] = track_names\n",
    "df['artists name'] = track_artists_name\n",
    "df['artists id'] = track_artists_id\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next:<br>\n",
    "From each track ID, will get the track features consisting of the following (Web API Reference | Spotify for Developers, n.d.):\n",
    "<ul>\n",
    "    <li><b>Accousticness</b> (A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.)</li>\n",
    "    <li><b>Danceability</b> (Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.)</li>\n",
    "    <li><b>Duration_ms</b> (The duration of the track in milliseconds.)</li>\n",
    "    <li><b>Energy</b> (Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.)</li>\n",
    "    <li><b>Instrumentalness</b> (Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.)</li>\n",
    "    <li><b>Key</b> (The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on.)</li>\n",
    "    <li><b>Liveness</b> (Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.)</li>\n",
    "    <li><b>Loudness</b> (The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.)</li>\n",
    "    <li><b>Mode</b> (Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.)</li>\n",
    "    <li><b>Speechiness</b> (Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.)</li>\n",
    "    <li><b>Time signature</b> (An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).)</li>\n",
    "    <li><b>Valence</b> (A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source this\n",
    "track_ids = df['id']\n",
    "track_acousticness = []\n",
    "track_danceability = []\n",
    "track_duration_ms = []\n",
    "track_energy = []\n",
    "track_instrumentalness = []\n",
    "track_key = []\n",
    "track_liveness = []\n",
    "track_loudness = []\n",
    "track_mode = []\n",
    "track_speechiness = []\n",
    "track_time_signature = []\n",
    "track_valence = []\n",
    "                                            \n",
    "for id in track_ids:\n",
    "    features = sp.audio_features(id)\n",
    "    acousticness = features[0]['acousticness']\n",
    "    danceability = features[0]['danceability']\n",
    "    duration_ms = features[0]['duration_ms']\n",
    "    energy = features[0]['energy']\n",
    "    instrumentalness = features[0]['instrumentalness']\n",
    "    key = features[0]['key']\n",
    "    liveness = features[0]['liveness']\n",
    "    loudness = features[0]['loudness']\n",
    "    mode = features[0]['mode']\n",
    "    speechiness = features[0]['speechiness']\n",
    "    time_signature = features[0]['time_signature']\n",
    "    valence = features[0]['valence']\n",
    "    \n",
    "    track_acousticness.append(acousticness)\n",
    "    track_danceability.append(danceability)\n",
    "    track_duration_ms.append(duration_ms)\n",
    "    track_energy.append(energy)\n",
    "    track_instrumentalness.append(instrumentalness)\n",
    "    track_key.append(key)\n",
    "    track_liveness.append(liveness)\n",
    "    track_loudness.append(loudness)\n",
    "    track_mode.append(mode)\n",
    "    track_speechiness.append(speechiness)\n",
    "    track_time_signature.append(time_signature)\n",
    "    track_valence.append(valence)\n",
    "    \n",
    "df['track acousticness'] = track_acousticness\n",
    "df['track danceability'] = track_danceability\n",
    "df['track duration_ms'] = track_duration_ms\n",
    "df['track energy'] = track_energy\n",
    "df['track instrumentalness'] = track_instrumentalness\n",
    "df['track key'] = track_key\n",
    "df['track liveness'] = track_liveness\n",
    "df['track loudness'] = track_loudness\n",
    "df['track mode'] = track_mode\n",
    "df['track speechiness'] = track_speechiness\n",
    "df['track time_signature'] = track_time_signature\n",
    "df['track valence'] = track_valence\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of gathering the data for this project, we'll get the genres from artist ids. The result might return a few genres per artist, therefore for this project, we use only the last genre with the assumption that it's the broadest genre. Then, we save them as a csv file to keep the data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source this\n",
    "artists_id = df['artists id']\n",
    "genres = []\n",
    "\n",
    "for artist in artists_id:\n",
    "    urn = 'spotify:artist:' + artist\n",
    "    artist = sp.artist(urn)\n",
    "    artist_genres = artist['genres']\n",
    "    \n",
    "    if len(artist_genres) != 0:\n",
    "        broad_genre = artist_genres[-1] #only the last genre with the assumption that it's the broadest genre, also otherwise too many genre present.\n",
    "    else:\n",
    "        broad_genre = 'no genre'\n",
    "    \n",
    "    genres.append(broad_genre)\n",
    "    \n",
    "df['genres'] = genres\n",
    "df.to_csv('raw_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we are working with the local dataset and continue with filtering all the genres to only show the ones containing pop and rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('raw_data.csv')  \n",
    "df2 = df1[df1['genres'].str.contains('pop|rock')]\n",
    "df2['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier for us to process the genre later on, we're going to transform the values of genres column to the following: when it is a sub genre of pop, we're turning it into 'pop' and when it is a sub genre of rock, we're turning it into 'rock'. The values will be put to a new column named genres-transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['genres'].str.contains('pop'), 'genres-transformed'] = 'pop'\n",
    "df2.loc[df2['genres'].str.contains('rock'), 'genres-transformed'] = 'rock'\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're making dummies out of the genres-transformed values to later on see if there are strong correlation values to each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = df2['genres-transformed'].str.get_dummies()\n",
    "df2 = pd.concat([df2, dummies], axis=1) \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe is ready to be processed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above dataset, not a lot of transformation happens to the data. We did have to undergo a few steps to actually get the genre: Track id > artist id > genre, while the track feature is relatively easy: Track id > Feature track. We transformed all the pop and rock sub genres into just 2 genres: pop and rock to be used for the prediction.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to look at most strongly correlating variables, possibly also to the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df2.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above correlation table and heatmap, there is no variable that is strongly correlated to the pop and rock genre. Let's now take a look at the track features correlation. We can see that the correlation between track energy and track loudness are highest at 0.76. Let's analyze these in the below graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"genres-transformed\", y=\"track energy\", kind=\"violin\", inner=None, data=df2)\n",
    "sns.swarmplot(x=\"genres-transformed\", y=\"track energy\", color=\"k\", size=3, data=df2, ax=g.ax)\n",
    "plt.xlabel('Genres') \n",
    "plt.ylabel('Track Energy')\n",
    "plt.title('Track Energy per Genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above, we can see that pop track energy is more grouped in 0.6 to 0.8 while track energy for pop is relatively more spread out from 0.4 to 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"genres-transformed\", y=\"track loudness\", kind=\"violin\", inner=None, data=df2)\n",
    "sns.swarmplot(x=\"genres-transformed\", y=\"track loudness\", color=\"k\", size=3, data=df2, ax=g.ax)\n",
    "plt.xlabel('Genres') \n",
    "plt.ylabel('Track Loudness')\n",
    "plt.title('Track Loudness per Genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we can see that pop track loudness is once again more grouped in around -5 while track loudness for rock is more spread out in range, from almost -25decibel to more than 0 decibel. The track loudness for rock is also a bit more even, grouped around -12 until around -6.<br>\n",
    "<br>\n",
    "From the above two graphs we can see that pop music are generally higher in the energy and loudness. Rock music energy and loudness are relatively more spread oud in comparison to pop music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='track energy', y='track loudness', fit_reg=True, data=df2, scatter_kws={'alpha':0.3})\n",
    "plt.xlabel('Track Energy') \n",
    "plt.ylabel('Track Loudness')\n",
    "plt.title('Track Energy in Comparison to Track Loudness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the track energy on the x axis and track loudness on the y axis. In this graph, we can analyze better the relationship between the two variables. The track energy that are low or from 0 to 0.4 have their track loudness until around -15 decibels. The track energy that are higher or more than 0.5, have their track loudness ranged from -15 to -2 decibels. The graph shows relatively linear distribution of dots, meaning that the track energy have linear relation to track loudness. The higher the track energy, the higher the track loudness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use Random Forest Classifier. Why random forest? Random forests consist of multiple single trees each based on a random sample of the training data. They are typically more accurate than single decision trees. Random forest also can be used to predict both classification and regression. For this project, classification will be predicted by predicting the track genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['genres-transformed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number above, rock genre has more data. It seems like it will be more difficult to predict pop genre. If we guess that a track is rock, we have roughly 72% accuracy. However, we will also take a look at the precision and recall as well in the later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the track features as the X variables. The X variables will be used to predict the genre.<br>\n",
    "<br>\n",
    "In the below cell, the training and test data are separated by 70% and 30%. This built-in function from sk-learn splits the data set randomly into a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.loc[:,'track acousticness':'track valence']\n",
    "y = df2['genres-transformed']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below random forest model is a built-in function from sk-learn. RF uses randomness, a random_state is set for stability of result. Traditionally, literature suggests 10 more trees to be used in this model. But in this project, we'll use 100 (see n_estimators below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
    "rf = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below, we're going to evaluate the model using a confusion matrix and calculating accuracy, precision and recall. This is typical for classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test) \n",
    "conf_matrix = confusion_matrix(y_test, y_pred) \n",
    "conf_matrix = pd.DataFrame(cm, index=['pop (actual)', 'rock (actual)'], columns = ['pop  (predicted)', 'rock (predicted)']) \n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the number of confusion matrix, rock is predicted better than pop. Now, let's calculate the accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually calculate the proportion of correctly predicted or accuracy, the formula is as follows:<br>\n",
    "<br>\n",
    "$ accuracy = \\frac{correctlyPredicted}{totalCases} $<br>\n",
    "<br>According to the formula, the accuracy of our prediction is 79%. That's quite good for predicting music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\")\n",
    "(16+110)/(16+10+24+110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually calculate the proportion of how much predicted is the actual or the precision, the formula is as follows:<br>\n",
    "<br>\n",
    "$ precision(var) = \\frac{correctlyPredicted(var)}{totalPrediction(var)} $<br>\n",
    "<br>According to the formula, the precision for pop is 62% and for rock is 82%. Good number for rock, but just okay for pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision for pop:\")\n",
    "(16)/(16+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision for rock:\")\n",
    "(110)/(24+110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually calculate the proportion of how much the actual is in the prediction or recall, the formula is as follows:<br>\n",
    "<br>\n",
    "$ recall(var) = \\frac{correctlyPredicted(var)}{totalActual(var)} $<br>\n",
    "<br>According to the formula, the recall for pop is 40% which is not that good, but the recall for rock is very good with 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall for pop:\")\n",
    "(16)/(16+24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall for rock:\")\n",
    "(110)/(10+110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the result through manual calculation, we'll calculate and analyze precision and recall with *classification_report*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the prediction is 79%, which is good for music prediction. The precision for rock music is very good with 0.82, while for pop is in the border of okay and not okay with 0.62. The recall for rock is great with 0.92 while the recall for rock is relatively bad with 0.40. As we can see, the precision and recall number is the same calculated manually and using _classification model_.<br>\n",
    "<br>\n",
    "<u>Let's say Spotify is to create a pop and rock playlist with the above calculation. If spotify only put tracks they know 100% sure are rock, they will get only rock tracks (high precision) and many too (high recall). If on the other hand, Spotify is to create pop playlist, they will have little amount of pop songs (low precision and low recall).</u>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking out variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When taking out one variable at a time from the model above, sometimes the precision and recall is up a little bit, around 0.02. From all calculation, when track speechiness is taken out of the X variables, it causes the most change in the precision and recall as follows: precision for pop is 0.66 and rock is 0.84 while recall for pop is 0.47 and rock is the same 0.92.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to try different parameters for our model. *N_estimators* is by default set to 10 and in our previous calculation, we set it to 100. Let's now use 1. The *max_features*, we're going to set it to the square root of the number variables, which is rounded up to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_new  = RandomForestClassifier(n_estimators = 1, max_features = 3, random_state=1) #RF is a random algorithm, so to get the same results we need to use random_state\n",
    "rf_new = rf_new.fit(X_train, y_train)\n",
    "y_pred_new = rf_new.predict(X_test) #the predicted values\n",
    "print(classification_report(y_test, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision, recall and accuracy are all at loss. The previous values are still better than the new parameters.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tried with KNN model, the accuracy is 67.5%. With 5 neighbours (out of 10 neighbours in the range), the precision and recall are generally the highest: precision for pop is 0.40 and rock is 0.78 while recall for pop is 0.30 and rock is the same 0.85. This means, even the best number in this model still are less than the result of Random Forest model that we use in the beginning.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "Web API Reference | Spotify for Developers. (n.d.). Spotify. Retrieved January 20, 2021, from https://developer.spotify.com/documentation/web-api/reference/#objects-index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
